{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23ff046-7af9-4f60-9bc7-04c752cd2394",
   "metadata": {},
   "source": [
    "# Project 3  - Federated Semantic Segmentation for self-driving cars\n",
    "The porpouse of the project is to familiarize with Federated Learning, which is a possible solution to perserve the privacy of the users while training a neural network in the context of Semantic Segmentation applied to surrounding recogniction for autonomous vehicles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a2fab-aa97-4a98-a264-24114cc26c85",
   "metadata": {},
   "source": [
    "## Imports\n",
    "In this first section, we import all the packages that we need in order for the code to work. If you want, you can create a virtual environment and install all the packages needed with the `requirements.txt` file.\n",
    "\n",
    "First of all, we load the IPython extension \"autoreload\", which allows us to modify the source code without the need to restart the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d18355-7987-4f30-a21b-b4f7af140a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a96238-194b-46db-94f1-7e43cebd5abe",
   "metadata": {},
   "source": [
    "We tried to use as much as possible Google Colab in orded to exploit it's GPU computational power, but with the free version we were costantly timed out for the day and so we decided to switch to a local GPU if Google Colab was not available. \n",
    "\n",
    "The following code configure Google Colab in case the notebook was runned there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa25f6-6205-4d5a-a306-93bac0b7f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/drive')\n",
    "    %pip install wandb\n",
    "    %cd /content/drive/MyDrive/DAAI\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f273f7a-be1e-4d57-a4c3-d0ae06966d7d",
   "metadata": {},
   "source": [
    "Then, we imported the modules that we needed. Some are standard python - or pytorch - modules, some are taken (and modified, if necessary) from public papers and some are written by us. You can find the complete references of the papers and code used in the official report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdddfa93-5125-4995-a0f6-ac265018ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.cityscapes import Cityscapes\n",
    "from dataset.gta5 import Gta5\n",
    "from clients.client import Client\n",
    "from clients.student import StudentClient\n",
    "from server.server import Server\n",
    "from modules.bisenetv2 import BiSeNetV2\n",
    "import dataset.transform as CT\n",
    "from dataset.utils import *\n",
    "from utils.style_transfer import StyleAugment\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361994b8-72c9-4ba0-800f-ff225ee13a35",
   "metadata": {},
   "source": [
    "We decided to use WandB to log our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7920ab-f16e-4241-aff1-76dac40a32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03207427-c0c6-4275-b14b-5678f4782856",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='configuration'></a>\n",
    "## Configuration\n",
    "In this section we set all the constants and parameters that we use in the following steps. Amongst the others, we set the hyperparameters for the neural networks (centralized and FL), the paths to the data, the configuration dictionary for the logger and the combination of transform used to train the network. Those are the custom transform for semantic segmentation, as suggested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f72f03-828a-43da-b937-026df5fe89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CITYSCAPES_PATH = os.path.join('data', 'Cityscapes')\n",
    "GTA5_PATH = os.path.join('data', 'GTA5')\n",
    "\n",
    "HEIGHT, WIDTH = 512, 1024\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # 'cuda' or 'cpu' or 'mps'\n",
    "\n",
    "\n",
    "# CENTRALIZED BASELINE ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "NUM_CLASSES = 19 # 19 classes defined in dataset/cityscapes\n",
    "\n",
    "BATCH_SIZE = 4  # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 100e-3            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-4  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 30     # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = 10       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.5        # Multiplicative factor for learning rate step-down (0.1 default)\n",
    "\n",
    "hypers = {\"LR\": LR,\n",
    "         \"MOMENTUM\": MOMENTUM,\n",
    "         \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "         \"STEP_SIZE\": STEP_SIZE,\n",
    "         \"GAMMA\": GAMMA\n",
    "         }\n",
    "\n",
    "wandb_config = {\"initial_lr\": LR,\n",
    "                \"step_size\": STEP_SIZE,\n",
    "                \"step_down_gamma\": GAMMA,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"epochs\": NUM_EPOCHS\n",
    "               }\n",
    "\n",
    "# FEDERATED LEARNING + SEMANTIC SEGMENTATION --------------------------------------------------------------------------------------\n",
    "\n",
    "BATCH_SIZE_FLSS = 2  # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR_FLSS = 100e-3            # The initial Learning Rate\n",
    "MOMENTUM_FLSS = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY_FLSS = 5e-4  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS_FLSS = 3    # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE_FLSS = 1     # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA_FLSS = 0.5         # Multiplicative factor for learning rate step-down (0.1 default)\n",
    "NUM_ROUNDS = 10\n",
    "NUM_CLIENTS = 50\n",
    "CLIENTS_PER_ROUND = 10\n",
    "\n",
    "HETEROGENEOUS = False\n",
    "FDA_WINDOW_SIZE = 32 # Half the size of FDA window\n",
    "\n",
    "hypers_flss = {\"LR\": LR_FLSS,\n",
    "         \"MOMENTUM\": MOMENTUM_FLSS,\n",
    "         \"WEIGHT_DECAY\": WEIGHT_DECAY_FLSS,\n",
    "         \"STEP_SIZE\": STEP_SIZE_FLSS,\n",
    "         \"GAMMA\": GAMMA_FLSS\n",
    "         }\n",
    "\n",
    "wandb_config_flss = {\n",
    "        \"initial_lr\": LR_FLSS,\n",
    "        \"step_size\": STEP_SIZE_FLSS,\n",
    "        \"step_down_gamma\": GAMMA_FLSS,\n",
    "        \"batch_size\": BATCH_SIZE_FLSS,\n",
    "        \"epochs\": NUM_ROUNDS,\n",
    "        \"split\": \"heterogeneous\" if HETEROGENEOUS else \"uniform\"\n",
    "}\n",
    "\n",
    "# AUGMENTATION\n",
    "\n",
    "train_transform = CT.Compose(\n",
    "    [CT.Resize([HEIGHT, WIDTH]),\n",
    "     CT.RandomHorizontalFlip(),\n",
    "     CT.ToTensor(),\n",
    "     \n",
    "    ])\n",
    "\n",
    "test_transform = CT.Compose(\n",
    "    [CT.Resize([HEIGHT, WIDTH]),\n",
    "     CT.RandomHorizontalFlip(),\n",
    "     CT.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1849907-624b-4b7c-ae88-afb1dceb3da0",
   "metadata": {},
   "source": [
    "<a id='datasets'></a>\n",
    "## 1. Partition Cityscapes into Train and Test\n",
    "First of all, we need to generate the datasets that are gonna be used for our experiments. In particular, with `DSA` and `DSB` we indicate the partition A and partition B referred in the project paper. For what concernes the partition A, we first check if it already exists a `.txt` file containing the images. If it does, we load it; if it doesn't, we generate one according to the project paper and saves it in a `.txt` file. This is done in order to have the same dataset thorugh all the following steps.\n",
    "\n",
    "You can find the code for all the classes and methods related to the dataset in the folder \"dataset\". In detail, we use the methods in \"utils\" to generate a list from a `.txt` file and one for the inverse operation. This is because we designed the `Cityscapes` class (in dataset/cityscapes.py) to be built starting from a list of image paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df74cd02-09ff-418b-a08b-3fbeb424f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset A - Loading the split if it exists, creating otherwise\n",
    "path_testA = os.path.join(CITYSCAPES_PATH, 'testA.txt')\n",
    "path_trainA = os.path.join(CITYSCAPES_PATH, 'trainA.txt')\n",
    "\n",
    "if os.path.isfile(path_testA) and os.path.isfile(path_trainA):\n",
    "    testA = txt2list(os.path.join(CITYSCAPES_PATH, 'testA.txt'))\n",
    "    trainA = txt2list(os.path.join(CITYSCAPES_PATH, 'trainA.txt'))\n",
    "    print(\"Dataset A found and loaded.\")\n",
    "\n",
    "else:\n",
    "    testA, trainA = random_split(os.path.join(CITYSCAPES_PATH, 'images'))\n",
    "    list2txt(testA, path_testA)\n",
    "    list2txt(trainA, path_trainA)\n",
    "    print(\"Dataset A not found - Generated one instead.\")\n",
    "\n",
    "\n",
    "DSA_train = Cityscapes(CITYSCAPES_PATH, trainA, transform=train_transform, cl19=True)\n",
    "DSA_test = Cityscapes(CITYSCAPES_PATH, testA, transform=test_transform, cl19=True)\n",
    "\n",
    "# Dataset B\n",
    "trainB = txt2list(os.path.join(CITYSCAPES_PATH, 'train.txt'))\n",
    "testB = txt2list(os.path.join(CITYSCAPES_PATH, 'val.txt'))\n",
    "\n",
    "DSB_train = Cityscapes(CITYSCAPES_PATH, trainB, transform=train_transform, cl19=True)\n",
    "DSB_test = Cityscapes(CITYSCAPES_PATH, testB, transform=test_transform, cl19=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a54436-42d1-440a-875a-156001683353",
   "metadata": {},
   "source": [
    "<a id='centralized'></a>\n",
    "## 2. Centralized Baseline\n",
    "In this step, we train centralized baselines for both the A/B partitions that is going to be used for the next steps. To do so, we used, as suggested, the BiSeNet V2 network, which we loaded from `modules.bisenetv2`. \n",
    "\n",
    "We used `torch.cuda.empty_cache()` due to the limited memory in the local GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06149e0-f3c1-4745-af01-3a807084d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Client\n",
    "torch.cuda.empty_cache()\n",
    "net = BiSeNetV2(NUM_CLASSES, output_aux = False)\n",
    "net.to(DEVICE);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66238529-0001-4a43-b433-c639fbeb2963",
   "metadata": {},
   "source": [
    "Now we are ready to run the first experiment and train the centralized network. In order to do so, having already in mind the Federated Learning setting, we decided to build a `Client` class and use that to train the network.\n",
    "\n",
    "The `Client` class is constructed from a network and a dataset on which the network will be trained. The dataloader is constructed automatically inside the client from the dataset. Moreover, it contains method to train and test the model, and also to save and load checkpoints. You can find the source code in `clients.client`.\n",
    "\n",
    "Since we use WandB as datalogger, we initialize a run and finish it after the network is trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66bf732-528b-410d-9136-1ebfb02cd696",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"baseline_DSA\", config=wandb_config)\n",
    "baseline_DSA = Client(f\"{wandb.run.name}-baseline-DSA\", DSA_train, net, BATCH_SIZE, DEVICE, NUM_EPOCHS, hypers, num_classes=NUM_CLASSES)\n",
    "baseline_DSA.train(NUM_EPOCHS, hypers)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b6bd9-ad2d-413f-b077-1c17d8efb104",
   "metadata": {},
   "source": [
    "We follow the same procedure for the partition B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66462917-7ae8-4658-b5fa-96ad03045cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "net = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "net.to(DEVICE);\n",
    "\n",
    "wandb.init(project=\"baseline_DSB\", config=wandb_config)\n",
    "baseline_DSB = Client(f\"{wandb.run.name}-baseline-DSB\", DSB_train, net, BATCH_SIZE, DEVICE, NUM_EPOCHS, hypers, num_classes=NUM_CLASSES)\n",
    "baseline_DSB.train(NUM_EPOCHS, hypers)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e03a8-9062-442a-b197-a1affc33b2ee",
   "metadata": {},
   "source": [
    "We ran this section several times, changing the hyperparameters and augmentation in order to find the best combination. You can find the best combination and the reasoning behind the choice in the official report.\n",
    "\n",
    "We can test the two best networks on the corrisponding partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e85701-2bba-4282-88aa-758cef748504",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = \"saved/centralized-DSA.pth\" # \"saved/centralized-DSA.pth\"; \"saved/centralized-DSB.pth\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "test_net = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "test_net.to(DEVICE)\n",
    "\n",
    "test_client = Client(f\"test_client\", DSA_test, test_net, BATCH_SIZE, DEVICE, NUM_EPOCHS, hypers, autosave=False)\n",
    "test_client.load_model(LOAD_PATH)\n",
    "\n",
    "accuracy = test_client.test()\n",
    "print(f\"Accuracy: {accuracy['Mean IoU']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0dddce-e610-4dac-abbe-a0d0167f3f87",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='step3'></a>\n",
    "## 3. Federated Semantic Segmentation experiments\n",
    "In this section, we run our first experiment combining Federated Learning and Semantic Segmentation.\n",
    "\n",
    "As suggested in the project paper, we tried to address the problems of Statistical Heterogeneity and Domain Generalization by distribuiting the data across the clients according to heterogeneous and uniform split. In any case, the test client contains the test dataset built in [step 1](#datasets).\n",
    "\n",
    "For code readability porpouses, the split is decided in the [configuration section](#configuration) and the experiment is runned different times, saving in WandB wether the split is heterogeneous or uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb654aa7-0943-41ff-a7a5-25117068290a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if HETEROGENEOUS:\n",
    "    split_A = heterogeneous_split(DSA_train, NUM_CLIENTS)\n",
    "    split_B = heterogeneous_split(DSB_train, NUM_CLIENTS)\n",
    "    \n",
    "else:\n",
    "    split_A = uniform_split(DSA_train, NUM_CLIENTS)\n",
    "    split_B = uniform_split(DSB_train, NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f709552e-2476-4a3d-8d14-02d0a2e81287",
   "metadata": {},
   "source": [
    "The procedure is similar to the one adopted in the [centralized baseline](#centralized), but in that case we used the `Server` class, available in `server.server`, to coordinate the training between the clients, exploiting to the Federated Averaging algorithm.\n",
    "\n",
    "In particular, when calling the `Server.fedAvg()` method, the server will:\n",
    "1. Select randomly a set of clients between the ones available\n",
    "1. Request an update to the set of clients sending the global model, in which each client will:\n",
    "    1. Train for a defined number of epochs on the local dataset\n",
    "    1. Return the updated local model parameters\n",
    "1. Average the updated local models through Federated Averaging algorithm\n",
    "1. Update the global model\n",
    "1. Repeat for a defined number of rounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf05ac-064b-42f3-a70a-dc4ad35190aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "net_flss = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "net_flss.to(DEVICE)\n",
    "\n",
    "clients = []\n",
    "for i, images_paths in enumerate(split_A):\n",
    "    client_dataset = Cityscapes(CITYSCAPES_PATH, images_paths, transform=train_transform, cl19=True)\n",
    "    client = Client(f\"c{i}\", client_dataset, net_flss, BATCH_SIZE_FLSS, DEVICE, NUM_EPOCHS_FLSS, hypers_flss, autosave=False)\n",
    "    clients.append(client)\n",
    "    \n",
    "\n",
    "wandb_config_flss[\"dataset\"] = \"Dataset A\"\n",
    "wandb.init(project=\"FL+SS\", config=wandb_config_flss)\n",
    "server = Server(f\"{wandb.run.name}-server\", net_flss, DEVICE, clients)\n",
    "server.run_fedAvg(NUM_ROUNDS)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34ce47-2b4b-4871-9d39-76bbc7f6af9a",
   "metadata": {},
   "source": [
    "The same procedure is used for the Dataset B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9fd77-c043-460b-9081-f4ec38ab23f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "net_flss = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "net_flss.to(DEVICE)\n",
    "\n",
    "clients = []\n",
    "for i, images_paths in enumerate(split_B):\n",
    "    client_dataset = Cityscapes(CITYSCAPES_PATH, images_paths, transform=train_transform, cl19=True)\n",
    "    client = Client(f\"c{i}\", client_dataset, net_flss, BATCH_SIZE_FLSS, DEVICE, NUM_EPOCHS_FLSS, hypers_flss, autosave=False)\n",
    "    clients.append(client)\n",
    "\n",
    "wandb_config_flss[\"dataset\"] = \"Dataset B\"\n",
    "wandb.init(project=\"FL+SS\", config=wandb_config_flss)\n",
    "server = Server(f\"{wandb.run.name}-server\", net_flss, DEVICE, clients)\n",
    "server.run_fedAvg(NUM_ROUNDS)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ecddf1-96ee-44e4-99d6-edb0a65985f9",
   "metadata": {},
   "source": [
    "We can perform some tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387db0f-7807-41d0-b1e4-05b934af7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = \"saved/FLSS-hetA.pth\" # \"saved/FLSS-hetA.pth\"; \"saved/FLSS-hetB.pth\"; \"saved/FLSS-uniA.pth\"; \"saved/FLSS-uniB.pth\";\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "test_net = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "test_net.to(DEVICE)\n",
    "\n",
    "test_client = Client(f\"test_client\", DSA_test, test_net, BATCH_SIZE, DEVICE, NUM_EPOCHS, hypers, autosave=False)\n",
    "test_client.load_model(LOAD_PATH)\n",
    "\n",
    "accuracy = test_client.test()\n",
    "print(f\"Accuracy: {accuracy['Mean IoU']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35c6db-9c07-454b-b9c4-4da1d43c5780",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "## 4. Moving towards FFreeDA - Pre-training phase\n",
    "In this step, we will move towards Source Free Domain Adaptation. The problem that we try to address is that usually the clients (such as self-driving) cars do not have access to labeled data. However, it is reasonable to assume that the server model is pre-trained on an open-source dataset such as GTA5.\n",
    "\n",
    "In this section we will implement the pre-training of the network, training the network from scratch and tuning again the hyperparameters and transforms. Since we want to test the model on images from the Cityscapes dataset, we built the `Gta5` class (available in `dataset.gta5`), which - for that porpouse - maps the labels making sure that the label class from GTA5 dataset are the same as Cityscapes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f691d2-55e0-4dda-800c-ab06fc15814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GTA5 = txt2list(os.path.join(GTA5_PATH, 'train.txt'))\n",
    "GTA5_train_dataset = Gta5(GTA5_PATH, GTA5, transform=train_transform)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "GTA5_net = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "GTA5_net.to(DEVICE);\n",
    "\n",
    "wandb_config[\"FDA\"] = False\n",
    "wandb.init(project=\"GTA5_train\", config=wandb_config)\n",
    "GTA5_client = Client(f\"{wandb.run.name}-GTA5\", GTA5_train_dataset, GTA5_net, BATCH_SIZE, DEVICE, NUM_EPOCHS, hypers, num_classes=NUM_CLASSES)\n",
    "GTA5_client.train(NUM_EPOCHS, hypers)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97263c7e-992b-4ed3-ab2b-a73a772a3114",
   "metadata": {},
   "source": [
    "We then manually saved the best checkpoint and tested it on partitions of Cityscapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48cb72-f6a9-49ab-9931-63e721d3b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = \"saved/4_2-GTA5.pth\" # \"saved/4_2-GTA5.pth\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "test_net = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "test_net.to(DEVICE)\n",
    "\n",
    "test_client = Client(f\"test_client\", DSB_test, test_net, BATCH_SIZE, DEVICE, NUM_EPOCHS, hypers, autosave=False)\n",
    "test_client.load_model(LOAD_PATH)\n",
    "\n",
    "accuracy = test_client.test()\n",
    "print(f\"Accuracy: {accuracy['Mean IoU']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e039b-c856-4b20-92da-7428f4d0cae4",
   "metadata": {},
   "source": [
    "### Fourier Domain Adaptation\n",
    "\n",
    "Since we are pre-training the model on a dataset whose domain is different from the one of the clients, we must assure that the Domain Generalization is satisfied. To do so, we exploit the Fourier Domain Adaptation algorithm, which uses a Fourier transform on the target domain (clients - Cityscapes) and extract a \"style\". Through an Inverse Fourier Transform, the style is then applied to the source domain (server - GTA5). In that way, we obtain a new image from the source databset with the style of the target one.\n",
    "\n",
    "To do so, we implemented the class `StyleAugment` (availabel in `utils.style_transfer`) from LADD paper. Again, you can find the complete reference in the official report.\n",
    "\n",
    "We use the `StyleAugment.apply_style()` method as an added transform to the image in `Gta5` class.\n",
    "\n",
    "For the hyperparmeters, we used the best ones find in the [previous step](#step4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f48f86-71e8-4d5f-9014-e48569afa890",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_bank = StyleAugment(size=(WIDTH, HEIGHT), b=FDA_WINDOW_SIZE) # b: half the dimension of the square FDA window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec7dd0-54d1-4df5-bb88-abceb4e2536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adding Cityscapes styles to bank:')\n",
    "split_A = heterogeneous_split(DSA_train, NUM_CLIENTS)\n",
    "clients = []\n",
    "for i, images_paths in enumerate(split_A):\n",
    "    net_flss = BiSeNetV2(NUM_CLASSES)\n",
    "    client_dataset = Cityscapes(CITYSCAPES_PATH, images_paths, transform=train_transform, cl19=True)\n",
    "    client = Client(f\"c{i}\", client_dataset, net_flss, BATCH_SIZE_FLSS, DEVICE, NUM_EPOCHS_FLSS, hypers_flss, autosave=False)\n",
    "    clients.append(client)\n",
    "    \n",
    "for client in clients:\n",
    "    style_bank.add_style(client.dataset)\n",
    "    \n",
    "GTA5 = txt2list(os.path.join(GTA5_PATH, 'train.txt'))\n",
    "GTA5_train_dataset = Gta5(GTA5_PATH, GTA5, transform=train_transform, fda_style_transform=style_bank.apply_style)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "GTA5_net = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "GTA5_net.to(DEVICE);\n",
    "\n",
    "wandb_config[\"FDA\"] = True\n",
    "wandb_config[\"dataset\"] = \"Dataset A\"\n",
    "wandb_config[\"FDA_size\"] = FDA_WINDOW_SIZE\n",
    "wandb.init(project=\"GTA5_train\", config=wandb_config)\n",
    "GTA5_client = Client(f\"{wandb.run.name}-GTA5\", GTA5_train_dataset, GTA5_net, BATCH_SIZE, DEVICE, NUM_EPOCHS, hypers, num_classes=NUM_CLASSES)\n",
    "GTA5_client.train(NUM_EPOCHS, hypers)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d0868-fb56-40f1-82f6-b30f6b68a0d9",
   "metadata": {},
   "source": [
    "We repeated the process for dataset B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbbfed-dd8b-4290-99a8-7cf68c6d911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adding Cityscapes styles to bank:')\n",
    "split_B = heterogeneous_split(DSB_train, NUM_CLIENTS)\n",
    "clients = []\n",
    "for i, images_paths in enumerate(split_B):\n",
    "    net_flss = BiSeNetV2(NUM_CLASSES)\n",
    "    client_dataset = Cityscapes(CITYSCAPES_PATH, images_paths, transform=train_transform, cl19=True)\n",
    "    client = Client(f\"c{i}\", client_dataset, net_flss, BATCH_SIZE_FLSS, DEVICE, NUM_EPOCHS_FLSS, hypers_flss, autosave=False)\n",
    "    clients.append(client)\n",
    "    \n",
    "for client in clients:\n",
    "    style_bank.add_style(client.dataset)\n",
    "    \n",
    "GTA5 = txt2list(os.path.join(GTA5_PATH, 'train.txt'))\n",
    "GTA5_train_dataset = Gta5(GTA5_PATH, GTA5, transform=train_transform, fda_style_transform=style_bank.apply_style)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "GTA5_net = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "GTA5_net.to(DEVICE);\n",
    "\n",
    "wandb_config_flss[\"FDA\"] = True\n",
    "wandb_config_flss[\"dataset\"] = \"Dataset B\"\n",
    "wandb_config[\"FDA_size\"] = FDA_WINDOW_SIZE\n",
    "wandb.init(project=\"GTA5_train\", config=wandb_config)\n",
    "GTA5_client = Client(f\"{wandb.run.name}-GTA5\", GTA5_train_dataset, GTA5_net, BATCH_SIZE, DEVICE, NUM_EPOCHS, hypers, num_classes=NUM_CLASSES)\n",
    "GTA5_client.train(NUM_EPOCHS, hypers)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c8a79-b00b-4472-99cc-d68b244d2f18",
   "metadata": {},
   "source": [
    "We then manually saved the best checkpoints for both partition A and partition B and tested them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e6667-8e27-41b7-a8be-c7a26ca97cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = \"saved/autosave/good-firebrand-29-GTA5/epoch28.pth\" # \"saved/4_4-DSA-FDA.pth\"; \"saved/4_4-DSB-FDA.pth\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "test_net = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "test_net.to(DEVICE)\n",
    "\n",
    "test_client = Client(f\"test_client\", DSB_test, test_net, BATCH_SIZE, DEVICE, NUM_EPOCHS, hypers, autosave=False)\n",
    "test_client.load_model(LOAD_PATH)\n",
    "\n",
    "accuracy = test_client.test()\n",
    "print(f\"Accuracy: {accuracy['Mean IoU']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be65c74-bd89-4aa5-8010-216499652b05",
   "metadata": {},
   "source": [
    "## 5. Federated self-training using pseudo-labels\n",
    "Now that we have pre-trained a model, we can use that as a \"teacher\" model to generate pseudo-labels that will be used as ground-truth labels by a \"student\" model for training. \n",
    "\n",
    "To do so, we created a new class, `StudentClient` (available at `clients.student`), which is a child of the `Client` class. In addition to the parent class, it can store the teacher model and perform a forward pass on that model for each batch of images used for training and will use the pseudo labels obtained from the teacher model, instead of the real labels, to calcuate the losses.\n",
    "\n",
    "We initially set both teacher and student model as the pre-trained model obtained in [step 4](#step4) and assign them to student clients for each partition and split of Cityscapes described in [step 3](#step3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3358f6-f751-49fe-b1c1-93eef2e81389",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = student_model = BiSeNetV2(NUM_CLASSES, output_aux = True).to(DEVICE)\n",
    "\n",
    "if HETEROGENEOUS:\n",
    "    split_A = heterogeneous_split(DSA_train, NUM_CLIENTS)\n",
    "    split_B = heterogeneous_split(DSB_train, NUM_CLIENTS)\n",
    "    \n",
    "else:\n",
    "    split_A = uniform_split(DSA_train, NUM_CLIENTS)\n",
    "    split_B = uniform_split(DSB_train, NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be31a4-ab66-48d1-b7cf-cd233b74bbb5",
   "metadata": {},
   "source": [
    "Referring to the project paper, we follow three strategies:\n",
    "1. Teacher model never updated\n",
    "1. Teacher model updated at beginnign of each round\n",
    "1. Teacher model updatet every T rounds\n",
    "\n",
    "These strategies are managed by the `Server` class: if we don't pass T or we pass `T=0`, the server will never update the teacher model of the clients. If we pass `T=1`, the server will update the teacher model every round, setting it to its own model. If we pass `T>1`, the server will update the teacher model T rounds, again with its own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6c18d-5221-4580-916c-3b9748320189",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152526c-e09a-4be4-bc49-40a89ce6710d",
   "metadata": {},
   "source": [
    "Now we can begin the self-training with pseudo-labels, running different experiments loading the best checkpoints from [step 4](#step4), first without FDA and then with FDA, as described in the project paper.\n",
    "\n",
    "For sake of simplicity and readability, we wrote the code just one time and ran it different times manually loading the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf42b62-7168-45f8-b014-937c04844ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated Self Training - Dataset A\n",
    "\n",
    "LOAD_PATH = \"saved/4_4-DSA-FDA.pth\" # \"saved/4_2-GTA5.pth\"; \"saved/4_4-DSA-FDA.pth\";\n",
    "\n",
    "clients = []\n",
    "for i, images_paths in tqdm(enumerate(split_A)):\n",
    "    client_dataset = Cityscapes(CITYSCAPES_PATH, images_paths, transform=train_transform, cl19=True)\n",
    "    client = StudentClient(f\"c{i}\", client_dataset, student_model, BATCH_SIZE_FLSS, DEVICE, NUM_EPOCHS_FLSS, hypers_flss, autosave=False, teacher=teacher_model)\n",
    "\n",
    "    client.load_teacher(LOAD_PATH)\n",
    "    client.load_model(LOAD_PATH)\n",
    "\n",
    "    clients.append(client)\n",
    "\n",
    "wandb_config_flss[\"dataset\"] = \"Dataset A\"\n",
    "wandb_config_flss[\"T\"] = T\n",
    "wandb_config_flss[\"checkpoint\"] = \"4.4\"\n",
    "wandb.init(project=\"FST\", config=wandb_config_flss)\n",
    "FST_server = Server(f\"{wandb.run.name}-FST\", teacher_model, DEVICE, clients)\n",
    "\n",
    "# Loading teacher model\n",
    "FST_server.load_model(LOAD_PATH)\n",
    "\n",
    "FST_server.run_fedAvg(NUM_ROUNDS, T, CLIENTS_PER_ROUND)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a59e1b-53c4-4082-818a-023a48c1c79a",
   "metadata": {},
   "source": [
    "We follow the same procedure for Dataset B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c8ac4-90a7-46ab-a7a9-304b249a227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated Self Training - Dataset B\n",
    "\n",
    "\n",
    "LOAD_PATH = \"saved/4_4-DSB-FDA.pth\" # \"saved/4_2-GTA5.pth\"; \"saved/4_4-DSB-FDA.pth\";\n",
    "\n",
    "clients = []\n",
    "for i, images_paths in tqdm(enumerate(split_B)):\n",
    "    client_dataset = Cityscapes(CITYSCAPES_PATH, images_paths, transform=train_transform, cl19=True)\n",
    "    client = StudentClient(f\"c{i}\", client_dataset, student_model, BATCH_SIZE_FLSS, DEVICE, NUM_EPOCHS_FLSS, hypers_flss, autosave=False, teacher=teacher_model)\n",
    "\n",
    "    # Loading corresponding checkpoint (splits B.I or B.II)\n",
    "    client.load_teacher(LOAD_PATH)\n",
    "    client.load_model(LOAD_PATH)\n",
    "\n",
    "    clients.append(client)\n",
    "\n",
    "wandb_config_flss[\"dataset\"] = \"Dataset B\"\n",
    "wandb_config_flss[\"T\"] = T\n",
    "wandb_config_flss[\"checkpoint\"] = \"4.4\"\n",
    "wandb.init(project=\"FST\", config=wandb_config_flss)\n",
    "FST_server = Server(f\"{wandb.run.name}-FST\", teacher_model, DEVICE, clients)\n",
    "\n",
    "# Loading teacher model\n",
    "FST_server.load_model(LOAD_PATH)\n",
    "\n",
    "FST_server.run_fedAvg(NUM_ROUNDS, T, CLIENTS_PER_ROUND)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192b79e-8df5-4eac-8e98-fb678b8f76a1",
   "metadata": {},
   "source": [
    "The following test is performed considering a network trained with the heterogeneous split of partition A with FDA, using the Cityscapes test dataset of partition A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b70d66-aa25-4523-bb4f-3963ff1d15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LOAD PATHS: (heterogeneous)\n",
    "\"saved/A-NoFDA-T0.pth\"\n",
    "\"saved/A-NoFDA-T1.pth\"\n",
    "\"saved/A-NoFDA-T5.pth\"\n",
    "\"saved/A-FDA-T0.pth\"\n",
    "\"saved/A-FDA-T1.pth\"\n",
    "\"saved/A-FDA-T5.pth\"\n",
    "\"saved/B-NoFDA-T0.pth\"\n",
    "\"saved/B-NoFDA-T1.pth\"\n",
    "\"saved/B-NoFDA-T5.pth\"\n",
    "\"saved/B-FDA-T0.pth\"\n",
    "\"saved/B-FDA-T1.pth\"\n",
    "\"saved/B-FDA-T5.pth\"\n",
    "\n",
    "'''\n",
    "\n",
    "LOAD_PATH = \"saved/A-NoFDA-T0.pth\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "test_net = BiSeNetV2(NUM_CLASSES, output_aux = True)\n",
    "test_net.to(DEVICE)\n",
    "\n",
    "test_client = Client(f\"test_client\", DSA_test, test_net, BATCH_SIZE, DEVICE, NUM_EPOCHS, hypers, autosave=False)\n",
    "test_client.load_model(LOAD_PATH)\n",
    "\n",
    "accuracy = test_client.test()\n",
    "print(f\"Accuracy: {accuracy['Mean IoU']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
